Identifying deep voice samples generated by AI has been challanging tasks since the introduction of large scale generative models that can mimic human voice and accent. In this regard, building frameworks to differentiate natural voice samples from AI generated audio is an essential task for an AI safety.

During Mid-Term project of Machine Learning ZoomCamp I have build and deployed a system that can identify artificial audio clip from natural by analyzing frequency response of the given audio and applying Boosting algorithms to classify sample.

How to run the .ipynb files, you will need to follow the steps below:
- build virtual environment via pipenv
- install the required libraries listed in requirements.txt
- pipenv install -r requirements.txt && pipenv lock && pipenv shell
- activate the environment and run ipynb notebooks under given environment

How to deploy the model with Docker container:
- docker build --force-rm -t deepvoice -f Dockerfile .
- docker run -it --rm -p 9696:9696 deepvoice
- change visibility of port 9696 to public under 'PORTS' in VS code
- test pipeline from client side by running cd /src/ && python -m client

References:
- Kolmogorov-Smirnov Test on significance of different spectral variables https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test
- Wilcoxon rank-sum statistic to test that two sets of measurements are drawn from the same distribution https://www.stat.auckland.ac.nz/~wild/ChanceEnc/Ch10.wilcoxon.pdf

